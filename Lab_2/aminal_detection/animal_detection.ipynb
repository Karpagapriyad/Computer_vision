{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'Lab_2/aminal_detection/data/animal_detection/meta-data/meta-data/sample_submission.csv'\n",
    "test = 'Lab_2/aminal_detection/data/animal_detection/meta-data/meta-data/test.csv'\n",
    "train = 'Lab_2/aminal_detection/data/animal_detection/meta-data/meta-data/train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_subdir = 'Lab_2/aminal_detection/data/animal_detection/train/train'\n",
    "all_test_subdir ='Lab_2/aminal_detection/data/animal_detection/test/test'\n",
    "\n",
    "train_image_counts = {path.split('/')[-1]: len(glob.glob(path + '/*.jpg')) for path in all_train_subdir}\n",
    "test_image_counts = {path.split('/')[-1]: len(glob.glob(path + '/*.jpg')) for path in all_test_subdir}\n",
    "all_data_df=train_image_counts.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in /Users/karpagapriyadhanraj/miniconda3/envs/computer-vision/lib/python3.10/site-packages (8.1.1)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /Users/karpagapriyadhanraj/miniconda3/envs/computer-vision/lib/python3.10/site-packages (from ultralytics) (3.8.2)\n",
      "Requirement already satisfied: numpy>=1.22.2 in /Users/karpagapriyadhanraj/miniconda3/envs/computer-vision/lib/python3.10/site-packages (from ultralytics) (1.24.3)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /Users/karpagapriyadhanraj/miniconda3/envs/computer-vision/lib/python3.10/site-packages (from ultralytics) (4.8.1.78)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /Users/karpagapriyadhanraj/miniconda3/envs/computer-vision/lib/python3.10/site-packages (from ultralytics) (10.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /Users/karpagapriyadhanraj/miniconda3/envs/computer-vision/lib/python3.10/site-packages (from ultralytics) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.23.0 in /Users/karpagapriyadhanraj/miniconda3/envs/computer-vision/lib/python3.10/site-packages (from ultralytics) (2.31.0)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /Users/karpagapriyadhanraj/miniconda3/envs/computer-vision/lib/python3.10/site-packages (from ultralytics) (1.11.4)\n",
      "Requirement already satisfied: torch>=1.8.0 in /Users/karpagapriyadhanraj/miniconda3/envs/computer-vision/lib/python3.10/site-packages (from ultralytics) (2.1.2)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /Users/karpagapriyadhanraj/miniconda3/envs/computer-vision/lib/python3.10/site-packages (from ultralytics) (0.16.2)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /Users/karpagapriyadhanraj/miniconda3/envs/computer-vision/lib/python3.10/site-packages (from ultralytics) (4.66.1)\n",
      "Requirement already satisfied: psutil in /Users/karpagapriyadhanraj/miniconda3/envs/computer-vision/lib/python3.10/site-packages (from ultralytics) (5.9.0)\n",
      "Requirement already satisfied: py-cpuinfo in /Users/karpagapriyadhanraj/miniconda3/envs/computer-vision/lib/python3.10/site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: thop>=0.1.1 in /Users/karpagapriyadhanraj/miniconda3/envs/computer-vision/lib/python3.10/site-packages (from ultralytics) (0.1.1.post2209072238)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /Users/karpagapriyadhanraj/miniconda3/envs/computer-vision/lib/python3.10/site-packages (from ultralytics) (2.1.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /Users/karpagapriyadhanraj/miniconda3/envs/computer-vision/lib/python3.10/site-packages (from ultralytics) (0.13.1)\n",
      "Requirement already satisfied: hub-sdk>=0.0.2 in /Users/karpagapriyadhanraj/miniconda3/envs/computer-vision/lib/python3.10/site-packages (from ultralytics) (0.0.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/karpagapriyadhanraj/miniconda3/envs/computer-vision/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/karpagapriyadhanraj/miniconda3/envs/computer-vision/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/karpagapriyadhanraj/miniconda3/envs/computer-vision/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (4.46.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/karpagapriyadhanraj/miniconda3/envs/computer-vision/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/karpagapriyadhanraj/miniconda3/envs/computer-vision/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (23.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/karpagapriyadhanraj/miniconda3/envs/computer-vision/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/karpagapriyadhanraj/miniconda3/envs/computer-vision/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/karpagapriyadhanraj/miniconda3/envs/computer-vision/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2022.7.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/karpagapriyadhanraj/miniconda3/envs/computer-vision/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/karpagapriyadhanraj/miniconda3/envs/computer-vision/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/karpagapriyadhanraj/miniconda3/envs/computer-vision/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/karpagapriyadhanraj/miniconda3/envs/computer-vision/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/karpagapriyadhanraj/miniconda3/envs/computer-vision/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2023.7.22)\n",
      "Requirement already satisfied: filelock in /Users/karpagapriyadhanraj/miniconda3/envs/computer-vision/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/karpagapriyadhanraj/miniconda3/envs/computer-vision/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (4.8.0)\n",
      "Requirement already satisfied: sympy in /Users/karpagapriyadhanraj/miniconda3/envs/computer-vision/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/karpagapriyadhanraj/miniconda3/envs/computer-vision/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/karpagapriyadhanraj/miniconda3/envs/computer-vision/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/karpagapriyadhanraj/miniconda3/envs/computer-vision/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (2023.12.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/karpagapriyadhanraj/miniconda3/envs/computer-vision/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/karpagapriyadhanraj/miniconda3/envs/computer-vision/lib/python3.10/site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/karpagapriyadhanraj/miniconda3/envs/computer-vision/lib/python3.10/site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Ultralytics YOLOv8.1.1 ðŸš€ Python-3.10.13 torch-2.1.2 CPU (Apple M2)\n",
      "Setup complete âœ… (8 CPUs, 8.0 GB RAM, 214.9/228.3 GB disk)\n",
      "\n",
      "OS                  macOS-14.1.1-arm64-arm-64bit\n",
      "Environment         Darwin\n",
      "Python              3.10.13\n",
      "Install             pip\n",
      "RAM                 8.00 GB\n",
      "CPU                 Apple M2\n",
      "CUDA                None\n",
      "\n",
      "matplotlib          âœ… 3.8.2>=3.3.0\n",
      "numpy               âœ… 1.24.3>=1.22.2\n",
      "opencv-python       âœ… 4.8.1.78>=4.6.0\n",
      "pillow              âœ… 10.1.0>=7.1.2\n",
      "pyyaml              âœ… 6.0.1>=5.3.1\n",
      "requests            âœ… 2.31.0>=2.23.0\n",
      "scipy               âœ… 1.11.4>=1.4.1\n",
      "torch               âœ… 2.1.2>=1.8.0\n",
      "torchvision         âœ… 0.16.2>=0.9.0\n",
      "tqdm                âœ… 4.66.1>=4.64.0\n",
      "psutil              âœ… 5.9.0\n",
      "py-cpuinfo          âœ… 9.0.0\n",
      "thop                âœ… 0.1.1-2209072238>=0.1.1\n",
      "pandas              âœ… 2.1.3>=1.1.4\n",
      "seaborn             âœ… 0.13.1>=0.11.0\n",
      "hub-sdk             âœ… 0.0.3>=0.0.2\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install ultralytics\n",
    "! yolo checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from collections import defaultdict, Counter\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Iterable\n",
    "import yaml\n",
    "\n",
    "import cv2\n",
    "import plotly.express as px\n",
    "from plotly import subplots\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/karpagapriyadhanraj/Desktop/EPITA/Computer vision/Computer_vision/Lab_2/aminal_detection'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = '/Users/karpagapriyadhanraj/Desktop/EPITA/Computer vision/Computer_vision/Lab_2/aminal_detection/data/animal/'\n",
    "MASTER_PATH = 'Lab_2/aminal_detection/data/output'\n",
    "DEBUG = False \n",
    "CPU = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LookupTable:\n",
    "    \"\"\"Vocabulary - Label lookup table (token <-> index).\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        token_to_index: Optional[Dict[str, int]] = None,\n",
    "        unknown_token: str = '<unk>',\n",
    "        add_unknown_token: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_to_index: Predefine token to index mappings.\n",
    "            unknown_token: Unknown token value.\n",
    "            add_unknown_token: Use unknown token.\n",
    "        \"\"\"\n",
    "        self._token_to_index = token_to_index\n",
    "        self._unknown_token = unknown_token\n",
    "        self._add_unknown_token = add_unknown_token\n",
    "\n",
    "        if self._token_to_index is None:\n",
    "            self._token_to_index = {}\n",
    "\n",
    "        if unknown_token not in self._token_to_index and add_unknown_token:\n",
    "            self._token_to_index[unknown_token] = len(self._token_to_index)\n",
    "\n",
    "        self._index_to_token = {v: k for k, v in self._token_to_index.items()}\n",
    "        self._next_index = len(self._token_to_index)\n",
    "\n",
    "    def add(self, token: str) -> int:\n",
    "        \"\"\"\n",
    "        Adds token to the lookup table if it does not already exist.\n",
    "        \n",
    "        Args:\n",
    "            token: Label (token)\n",
    "            \n",
    "        Returns:\n",
    "            label (token) index\n",
    "        \"\"\"\n",
    "        if token in self._token_to_index:\n",
    "            return self._token_to_index[token]\n",
    "\n",
    "        new_index = self._next_index\n",
    "        self._next_index += 1\n",
    "        self._token_to_index[token] = new_index\n",
    "        self._index_to_token[new_index] = token\n",
    "        return new_index\n",
    "\n",
    "    def lookup(self, token: str) -> int:\n",
    "        \"\"\"\n",
    "        Acquires token index if it exists in the table.\n",
    "        In case the token does not exist in the lookup table:\n",
    "            - and unknown token is used then unkown token index is returned;\n",
    "            - otherwise KeyError is raised\n",
    "            \n",
    "        Raises:\n",
    "            KeyError: Unknown token\n",
    "            \n",
    "        Returns:\n",
    "            label (token) index\n",
    "        \"\"\"\n",
    "        if token not in self._token_to_index and self._add_unknown_token:\n",
    "            return self._token_to_index[self._unknown_token]\n",
    "\n",
    "        return self._token_to_index[token]\n",
    "\n",
    "    def inverse_lookup(self, index: int) -> str:\n",
    "        \"\"\"\n",
    "        Inverse to `lookup`. Acquire token by index.\n",
    "        \n",
    "        Raises:\n",
    "            KeyError: Unknown index\n",
    "            \n",
    "        Returns:\n",
    "            label (token)\n",
    "        \"\"\"\n",
    "        return self._index_to_token[index]\n",
    "    \n",
    "    def __iter__(self) -> Iterable[Tuple[str, int]]:\n",
    "        for token, index in self._token_to_index.items():\n",
    "            yield token, index\n",
    "\n",
    "    def __getitem__(self, token: str) -> int:\n",
    "        return self.lookup(token)  # Alias for `lookup`\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._next_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/karpagapriyadhanraj/Desktop/EPITA/Computer vision/Computer_vision/Lab_2/aminal_detection/data/animal/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 230\u001b[0m\n\u001b[1;32m    226\u001b[0m                     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(new_image_path):  \n\u001b[1;32m    227\u001b[0m                         shutil\u001b[39m.\u001b[39mcopy(image_path, new_image_path)\n\u001b[0;32m--> 230\u001b[0m adapter \u001b[39m=\u001b[39m AnimalToYOLODatasetAdapter(\n\u001b[1;32m    231\u001b[0m     path\u001b[39m=\u001b[39;49mDATASET_PATH, \n\u001b[1;32m    232\u001b[0m     label_filter\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mhorse\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39mif\u001b[39;49;00m DEBUG \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m    233\u001b[0m )\n\u001b[1;32m    235\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTotal number of samples in the dataset is \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(adapter)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    236\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTotal number of classes in the dataset is \u001b[39m\u001b[39m{\u001b[39;00madapter\u001b[39m.\u001b[39mn_labels\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 16\u001b[0m, in \u001b[0;36mAnimalToYOLODatasetAdapter.__init__\u001b[0;34m(self, path, label_filter)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m    path: Path where dataset is stored\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39m    label_filter: Use specific set of labels (remove others from dataset)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_path \u001b[39m=\u001b[39m path\n\u001b[1;32m     15\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel_stats, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit_stats, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel_lookup, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_size \u001b[39m=\u001b[39m \\\n\u001b[0;32m---> 16\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_index_dataset(path, label_filter)\n",
      "Cell \u001b[0;32mIn[11], line 38\u001b[0m, in \u001b[0;36mAnimalToYOLODatasetAdapter._index_dataset\u001b[0;34m(path, label_filter)\u001b[0m\n\u001b[1;32m     35\u001b[0m lookup \u001b[39m=\u001b[39m LookupTable(add_unknown_token\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     36\u001b[0m size \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> 38\u001b[0m splits \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mlistdir(path)\n\u001b[1;32m     39\u001b[0m \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m splits:        \n\u001b[1;32m     40\u001b[0m     split_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(path, split)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/karpagapriyadhanraj/Desktop/EPITA/Computer vision/Computer_vision/Lab_2/aminal_detection/data/animal/'"
     ]
    }
   ],
   "source": [
    "DatasetIndex = Dict[str, Dict[str, List[str]]]\n",
    "DatasetStats = Dict[str, int]\n",
    "\n",
    "\n",
    "class AnimalToYOLODatasetAdapter:\n",
    "    \"\"\"Adapts custom animal dataset to YOLO format.\"\"\"\n",
    "    def __init__(self, path: str, label_filter: Optional[List[str]] = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            path: Path where dataset is stored\n",
    "            label_filter: Use specific set of labels (remove others from dataset)\n",
    "        \"\"\"\n",
    "        self._path = path\n",
    "        \n",
    "        self._index, self.label_stats, self.split_stats, self.label_lookup, self._size = \\\n",
    "            self._index_dataset(path, label_filter)\n",
    "        \n",
    "    @staticmethod\n",
    "    def _index_dataset(path: str, label_filter: Optional[List[str]] = None) \\\n",
    "        -> Tuple[DatasetIndex, DatasetStats, DatasetStats, LookupTable, int]:\n",
    "        \"\"\"\n",
    "        Creates datast index. Index is mapping (split -> label -> sample_id). \n",
    "        Input dataset format is given in previosly defined structure.\n",
    "\n",
    "        Args:\n",
    "            path: Dataset path\n",
    "            label_filter: Filter used labels\n",
    "\n",
    "        Returns:\n",
    "            Dataset index, Label stats, Split stats, dataset size\n",
    "        \"\"\"\n",
    "        index: DatasetIndex = defaultdict(dict)\n",
    "        label_stats: DatasetStats = Counter()\n",
    "        split_stats: DatasetStats = Counter()\n",
    "        lookup = LookupTable(add_unknown_token=False)\n",
    "        size = 0\n",
    "\n",
    "        splits = os.listdir(path)\n",
    "        for split in splits:        \n",
    "            split_path = os.path.join(path, split)\n",
    "            labels = os.listdir(split_path)\n",
    "            for label in tqdm(labels, desc=f'Indexing {split}', unit='sample'):\n",
    "                if label_filter is not None and label not in label_filter:\n",
    "                    continue\n",
    "                \n",
    "                label_path = os.path.join(split_path, label)\n",
    "                sample_ids = [Path(filename).stem for filename in os.listdir(label_path) \n",
    "                              if filename != 'Label' and filename.endswith('.jpg')]\n",
    "                annotations_path = os.path.join(label_path, 'Label')\n",
    "                annot_sample_ids = [Path(filename).stem for filename in os.listdir(annotations_path)\n",
    "                                    if filename.endswith('.txt')]\n",
    "                assert set(sample_ids) == set(annot_sample_ids), 'Image sample ids and annotation sample ids do not match'\n",
    "\n",
    "                # Update index, stats and lookup\n",
    "                index[split][label] = sample_ids\n",
    "                \n",
    "                n_samples = len(sample_ids)\n",
    "                label_stats[label] += n_samples\n",
    "                split_stats[split] += n_samples\n",
    "                size += n_samples\n",
    "                \n",
    "                lookup.add(label)\n",
    "\n",
    "        return dict(index), dict(label_stats), dict(split_stats), lookup, size\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self._size\n",
    "    \n",
    "    @property\n",
    "    def labels(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            List of labels (classes) in lookup table\n",
    "        \"\"\"\n",
    "        return list(self.label_lookup)\n",
    "\n",
    "    @property\n",
    "    def n_labels(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            Number of labels (classes) in lookup table\n",
    "        \"\"\"\n",
    "        return len(self.label_lookup)\n",
    "    \n",
    "    def get_random_samples(self, n: int, split: str = 'train') -> List[Tuple[str, str, str]]:\n",
    "        \"\"\"\n",
    "        Fetchen `n` random samples from dataset for chosen split.\n",
    "        \n",
    "        Args:\n",
    "            n: Number of samples\n",
    "            split: chosen split\n",
    "            \n",
    "        Returns:\n",
    "            List of tuples (split, label, sample_id)\n",
    "        \"\"\"\n",
    "        split_index = self._index[split]\n",
    "        label_names, _ = zip(*self.labels)\n",
    "        \n",
    "        result: List[Tuple[str, str, str]] = []\n",
    "        for i in range(n):\n",
    "            label = random.choice(label_names)\n",
    "            sample_ids = split_index[label]\n",
    "            sample_id = random.choice(sample_ids)\n",
    "            result.append((split, label, sample_id))\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    def get_split_size(self, split: str) -> int:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            Number of samples in split\n",
    "        \"\"\"\n",
    "        return self.split_stats[split]\n",
    "\n",
    "    def get_image_path(self, split: str, label: str, sample_id: str) -> str:\n",
    "        \"\"\"\n",
    "        Animal dataset image path convention.\n",
    "        \n",
    "        Args:\n",
    "            split: Split\n",
    "            label: Label (token)\n",
    "            sample_id: Sample id\n",
    "        \n",
    "        Returns:\n",
    "            Image path\n",
    "        \"\"\"\n",
    "        return os.path.join(self._path, split, label, f'{sample_id}.jpg')\n",
    "\n",
    "    def load_image(self, split: str, label: str, sample_id: str) -> str:\n",
    "        \"\"\"        \n",
    "        Args:\n",
    "            split: Split\n",
    "            label: Label (token)\n",
    "            sample_id: Sample id\n",
    "        \n",
    "        Returns:\n",
    "            Loaded image\n",
    "        \"\"\"\n",
    "        image_path = self.get_image_path(split, label, sample_id)\n",
    "        if not os.path.exists(image_path):\n",
    "            raise FileNotFound(f'Image \"{image_path}\" not found!')\n",
    "        return cv2.imread(image_path)\n",
    "\n",
    "    def get_annot_path(self, split: str, label: str, sample_id: str) -> str:\n",
    "        \"\"\"\n",
    "        Animal dataset annotation path convention.\n",
    "        \n",
    "        Args:\n",
    "            split: Split\n",
    "            label: Label (token)\n",
    "            sample_id: Sample id\n",
    "        \n",
    "        Returns:\n",
    "            Annotation path\n",
    "        \"\"\"\n",
    "        return os.path.join(self._path, split, label, 'Label', f'{sample_id}.txt')\n",
    "\n",
    "    def parse_annot(self, split: str, label: str, sample_id: str) \\\n",
    "        -> List[Tuple[str, float, float, float, float]]:\n",
    "        \"\"\"        \n",
    "        Parses annotation (ground truth) file.\n",
    "        \n",
    "        Args:\n",
    "            split: Split\n",
    "            label: Label (token)\n",
    "            sample_id: Sample id\n",
    "        \n",
    "        Returns:\n",
    "            Parsed annotations\n",
    "        \"\"\"\n",
    "        annot_path = self.get_annot_path(split, label, sample_id)\n",
    "        with open(annot_path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        annots: List[Tuple[str, float, float, float, float]] = []\n",
    "        for l in lines:\n",
    "            items = l.split()\n",
    "            label_name = ' '.join(items[:-4])\n",
    "            coords = [float(v) for v in items[-4:]]\n",
    "            annots.append([label_name, *coords])\n",
    "        return annots\n",
    "    \n",
    "    def convert(self, path: str) -> None:\n",
    "        \"\"\"\n",
    "        Converts dataset tp YOLO format.\n",
    "        \n",
    "        Args:\n",
    "            path: Output path\n",
    "        \"\"\"\n",
    "        for split in self._index:\n",
    "            split_path = os.path.join(path, split)\n",
    "            images_path = os.path.join(split_path, 'images')\n",
    "            labels_path = os.path.join(split_path, 'labels')\n",
    "            Path(images_path).mkdir(parents=True, exist_ok=True)\n",
    "            Path(labels_path).mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            for label, sample_ids in tqdm(self._index[split].items(), desc='Converting to Yolo format', total=len(self._index[split])):\n",
    "                assert len(sample_ids) == len(set(sample_ids))\n",
    "                for sample_id in sample_ids:\n",
    "                    image_path = self.get_image_path(split, label, sample_id)\n",
    "                    new_image_path = os.path.join(images_path, f'{sample_id}.jpg')\n",
    "                    annots = self.parse_annot(split, label, sample_id)\n",
    "                    new_annot_path = os.path.join(labels_path, f'{sample_id}.txt')\n",
    "                    \n",
    "                    # Image needs to be loaded in order to read width and height\n",
    "                    # which are required for coordinate normalization\n",
    "                    image = self.load_image(split, label, sample_id)\n",
    "                    h, w, _ = image.shape\n",
    "                    \n",
    "                    # Conversion\n",
    "                    converted_annot: List[Tuple[int, float, float, float, float]] = []\n",
    "                    for label, x_min, y_min, x_max, y_max in annots:\n",
    "                        label_index = self.label_lookup[label]\n",
    "                        x_center = (x_min + x_max) / (2 * w)\n",
    "                        y_center = (y_min + y_max) / (2 * h)\n",
    "                        width = (x_max - x_min) / w\n",
    "                        height = (y_max - y_min) / h\n",
    "                        \n",
    "                        converted_annot.append((label_index, x_center, y_center, width, height))\n",
    "                        \n",
    "                    # Save data\n",
    "                    with open(new_annot_path, 'a', encoding='utf-8') as f:\n",
    "                        converted_annot_lines = [' '.join([str(v) for v in row]) for row in converted_annot]\n",
    "                        f.write('\\n'.join(converted_annot_lines))\n",
    "                        f.write('\\n')\n",
    "                        \n",
    "                    if not os.path.exists(new_image_path):  \n",
    "                        shutil.copy(image_path, new_image_path)\n",
    "\n",
    "\n",
    "adapter = AnimalToYOLODatasetAdapter(\n",
    "    path=DATASET_PATH, \n",
    "    label_filter=['horse'] if DEBUG else None\n",
    ")\n",
    "\n",
    "print(f'Total number of samples in the dataset is {len(adapter)}.')\n",
    "print(f'Total number of classes in the dataset is {adapter.n_labels}.')\n",
    "print(f'Train dataset size is {adapter.get_split_size(\"train\")} (images). Test dataset size is {adapter.get_split_size(\"test\")} (images)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'adapter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fig \u001b[39m=\u001b[39m px\u001b[39m.\u001b[39mhistogram(x\u001b[39m=\u001b[39m\u001b[39mlist\u001b[39m(adapter\u001b[39m.\u001b[39mlabel_stats\u001b[39m.\u001b[39mkeys()), y\u001b[39m=\u001b[39m\u001b[39mlist\u001b[39m(adapter\u001b[39m.\u001b[39mlabel_stats\u001b[39m.\u001b[39mvalues())) \\\n\u001b[1;32m      2\u001b[0m         \u001b[39m.\u001b[39mupdate_layout(xaxis_title\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mClass\u001b[39m\u001b[39m\"\u001b[39m, yaxis_title\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mClass size\u001b[39m\u001b[39m\"\u001b[39m, xaxis\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mcategoryorder\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39mtotal descending\u001b[39m\u001b[39m'\u001b[39m})\n\u001b[1;32m      3\u001b[0m fig\u001b[39m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'adapter' is not defined"
     ]
    }
   ],
   "source": [
    "fig = px.histogram(x=list(adapter.label_stats.keys()), y=list(adapter.label_stats.values())) \\\n",
    "        .update_layout(xaxis_title=\"Class\", yaxis_title=\"Class size\", xaxis={'categoryorder':'total descending'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_samples(\n",
    "    adapter: AnimalToYOLODatasetAdapter,\n",
    "    n_rows: int,\n",
    "    n_cols: int,\n",
    "    bbox_color: Tuple[int, int, int] = (255, 0, 0),  # RBG - RED\n",
    "    model: Optional[YOLO] = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Visualizes image sample with ground truths and (optionally) model predictions.\n",
    "    Number of images is equal to product of `n_rows` and `n_cols`\n",
    "    \n",
    "    Args:\n",
    "        adapter: Animal dataset to YOLO adapter\n",
    "        n_rows: Number of rows in image matrix\n",
    "        n_cols: Number of cols in image matrix\n",
    "        bbox_color: Ground truth bbox color\n",
    "        model: Model to generate prediction for given images\n",
    "    \"\"\"\n",
    "    n: int = n_rows * n_cols\n",
    "    \n",
    "    viz_samples = adapter.get_random_samples(n)\n",
    "    fig = subplots.make_subplots(rows=n_rows, cols=n_cols)\n",
    "    for plot_index, (split, label, sample_id) in enumerate(viz_samples):\n",
    "        row = plot_index // n_cols + 1\n",
    "        col = plot_index % n_cols + 1\n",
    "        image = adapter.load_image(split, label, sample_id)\n",
    "        label_index = adapter.label_lookup.lookup(label)\n",
    "        \n",
    "        # Visualize ground truth\n",
    "        for _, x_min, y_min, x_max, y_max in adapter.parse_annot(split, label, sample_id):\n",
    "            x_min, y_min, x_max, y_max = [int(v) for v in [x_min, y_min, x_max, y_max]]\n",
    "            labek_text = f'{label} ({label_index})'\n",
    "            image = cv2.rectangle(image, (x_min, y_min), (x_max, y_max), color=bbox_color, thickness=4)\n",
    "            image = cv2.putText(image, label, (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX, 2, bbox_color, 3)\n",
    "            \n",
    "        if model is not None:\n",
    "            # Visualize model predictions\n",
    "            prediction = model.predict([image], imgsz=512, conf=0.3)\n",
    "            for p in prediction:\n",
    "                image = p.plot()\n",
    "            \n",
    "        subfig = px.imshow(image)\n",
    "        fig.add_trace(subfig.data[0], row=row, col=col)\n",
    "\n",
    "    fig = fig.update_xaxes(showticklabels=False)\n",
    "    fig = fig.update_yaxes(showticklabels=False)\n",
    "    fig = fig.update_layout(\n",
    "        autosize=False,\n",
    "        width=1200,\n",
    "        height=600\n",
    "    )\n",
    "    fig.show()\n",
    "    \n",
    "visualize_samples(adapter, 1, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computer-vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
